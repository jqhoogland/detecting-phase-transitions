{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8U839iGXCH3",
        "outputId": "88f12940-8c9e-4cb7-c312-dabd92be351e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /home/paperspace/.local/lib/python3.9/site-packages (1.13.1)\n",
            "Requirement already satisfied: torchvision in /home/paperspace/.local/lib/python3.9/site-packages (0.14.1)\n",
            "Requirement already satisfied: livelossplot in /home/paperspace/.local/lib/python3.9/site-packages (0.5.5)\n",
            "Requirement already satisfied: wandb in /home/paperspace/.local/lib/python3.9/site-packages (0.13.4)\n",
            "Requirement already satisfied: torchtyping in /home/paperspace/.local/lib/python3.9/site-packages (0.1.4)\n",
            "Requirement already satisfied: tqdm in /home/paperspace/.local/lib/python3.9/site-packages (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /home/paperspace/.local/lib/python3.9/site-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/paperspace/.local/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/paperspace/.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/paperspace/.local/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/paperspace/.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: wheel in /home/paperspace/.local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.35.1)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (45.2.0)\n",
            "Requirement already satisfied: requests in /home/paperspace/.local/lib/python3.9/site-packages (from torchvision) (2.28.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/paperspace/.local/lib/python3.9/site-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: numpy in /home/paperspace/.local/lib/python3.9/site-packages (from torchvision) (1.24.1)\n",
            "Requirement already satisfied: matplotlib in /home/paperspace/.local/lib/python3.9/site-packages (from livelossplot) (3.6.3)\n",
            "Requirement already satisfied: bokeh in /home/paperspace/.local/lib/python3.9/site-packages (from livelossplot) (3.1.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (3.1.30)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (1.13.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setproctitle in /home/paperspace/.local/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /home/paperspace/.local/lib/python3.9/site-packages (from torchtyping) (3.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/paperspace/.local/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paperspace/.local/lib/python3.9/site-packages (from requests->torchvision) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/paperspace/.local/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/paperspace/.local/lib/python3.9/site-packages (from requests->torchvision) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/paperspace/.local/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /home/paperspace/.local/lib/python3.9/site-packages (from typeguard>=2.11.1->torchtyping) (6.0.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /home/paperspace/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (6.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /home/paperspace/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (23.0)\n",
            "Requirement already satisfied: contourpy>=1 in /home/paperspace/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /home/paperspace/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (3.1.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /home/paperspace/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (2023.2.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /home/paperspace/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (1.5.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/paperspace/.local/lib/python3.9/site-packages (from matplotlib->livelossplot) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/paperspace/.local/lib/python3.9/site-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/paperspace/.local/lib/python3.9/site-packages (from matplotlib->livelossplot) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/paperspace/.local/lib/python3.9/site-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/paperspace/.local/lib/python3.9/site-packages (from matplotlib->livelossplot) (4.38.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/paperspace/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/paperspace/.local/lib/python3.9/site-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping) (3.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/paperspace/.local/lib/python3.9/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/paperspace/.local/lib/python3.9/site-packages (from pandas>=1.2->bokeh->livelossplot) (2022.7.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision livelossplot wandb torchtyping tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMX1Jjrj0CzB"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# Add spectroscope to the path\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k66YJP740uCR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from livelossplot import PlotLosses\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from spectroscope.data import SubsetsLoader, get_filtered_dataset\n",
        "\n",
        "DEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "mnist_train = MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = MNIST('./data', train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr0ejX6YM7ej"
      },
      "source": [
        "## Simple ConvNet\n",
        "\n",
        "This an unmodified mnist learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cTkt86amTqCP"
      },
      "outputs": [],
      "source": [
        "class MNISTConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1)\n",
        "        self.fc1 = nn.Linear(512, 10)\n",
        "\n",
        "        self.to(DEVICE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkWi8PD9pEGx"
      },
      "source": [
        "# START OF IDEA: Introduction of new digits in training\n",
        "\n",
        "The section aims to see if introducing new digits later in the Epoch gives discontinuous boosts in peformance. I expect it shouldn't, since if we have the model say learn just 0 and 1 in the first half of training, then introduce the number 2, the parameters will be fitted only to (0,1) and do terrible on 2, so I expect a massive dip in accuracy in the middle, then some recovery at the end once it's seen some 0,1,2s but nowhere near the 90%+ as if we just did all 10 from the start. It's highly possible I may have misunderstood the setup, since I don't see how this could give a phase transition in decrease of loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkWDKPrZWCOv"
      },
      "source": [
        "The code below is simply testing the filter digit function does as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ux2HiG7QPlA",
        "outputId": "45a8af81-62ee-47d5-8781-8802f03a209f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SubsetsLoader((Subset(MNIST, len=5923, labels=(0,)), Subset(MNIST, len=12665, labels=(0, 1)), Subset(MNIST, len=18623, labels=(0, 1, 2)), Subset(MNIST, len=24754, labels=(0, 1, 2, 3)), Subset(MNIST, len=30596, labels=(0, 1, 2, 3, 4)), Subset(MNIST, len=36017, labels=(0, 1, 2, 3, 4, 5)), Subset(MNIST, len=41935, labels=(0, 1, 2, 3, 4, 5, 6)), Subset(MNIST, len=48200, labels=(0, 1, 2, 3, 4, 5, 6, 7)), Subset(MNIST, len=54051, labels=(0, 1, 2, 3, 4, 5, 6, 7, 8)), Subset(MNIST, len=60000, labels=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))), batch_size=256)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from typing import Any, Tuple, Type\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchtyping import TensorType\n",
        "\n",
        "\n",
        "def as_subset(cls: Type[Dataset]):\n",
        "    \"\"\"\n",
        "    Wrapper for a dataset class that adds a labels attribute to the dataset,\n",
        "    and a __repr__ method that prints the labels.\n",
        "    \"\"\"\n",
        "    class Subset(cls):\n",
        "        def __init__(self, data, targets, train: bool, labels: Tuple[int, ...], **kwargs):\n",
        "            self.labels = labels\n",
        "            self.data = data\n",
        "            self.targets = targets\n",
        "\n",
        "            self.train = train\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f\"Subset({cls.__name__}, len={len(self)}, labels={self.labels})\"\n",
        "        \n",
        "        def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "            \"\"\"\n",
        "            Args:\n",
        "                index (int): Index\n",
        "\n",
        "            Returns:\n",
        "                tuple: (image, target) where target is index of the target class.\n",
        "            \"\"\"\n",
        "            return self.data[index], int(self.targets[index])\n",
        "    \n",
        "    return Subset\n",
        "\n",
        "\n",
        "def filter_by_labels(dataset: Dataset, labels: Tuple[int, ...]):\n",
        "    \"\"\"\n",
        "    Returns an iterator over the dataset, yielding only the images and labels in labels.\n",
        "    \"\"\"\n",
        "    for image, label in dataset:\n",
        "        if label in labels:\n",
        "            yield image, label\n",
        "\n",
        "\n",
        "def get_filtered_dataset(dataset: Dataset, labels: Tuple[int, ...]):\n",
        "    \"\"\"\n",
        "    Returns a new dataset with only the images and labels in labels.\n",
        "    \"\"\"\n",
        "    dataset_cls = as_subset(type(dataset))\n",
        "    data, targets = zip(*((x, y) for x, y in filter_by_labels(dataset, labels)))\n",
        "    return dataset_cls(data, targets, train=dataset.train, labels=labels)  # type: ignore\n",
        "\n",
        "\n",
        "class SubsetsLoader:\n",
        "    \"\"\"\n",
        "    A data loader that trains on different subsets of the dataset \n",
        "    depending on the active subset.\n",
        "\n",
        "    Note: it's up to you to call next_subset() when you want to move to the next subset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        subsets: Tuple[Dataset],\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.subsets = subsets\n",
        "        self.subset_idx = 0\n",
        "        self.subloaders = tuple(DataLoader(subset, *args, **kwargs) for subset in subsets)\n",
        "\n",
        "    def next_subset(self):\n",
        "        \"\"\"Moves to the next subset.\"\"\"\n",
        "        if self.subset_idx < len(self.subsets):\n",
        "            self.subset_idx += 1\n",
        "        else:\n",
        "            raise StopIteration\n",
        "         \n",
        "    def to_subset(self, subset_idx: int):\n",
        "        \"\"\"Moves to the subset with index subset_idx.\"\"\"\n",
        "        if subset_idx < len(self.subsets):\n",
        "            self.subset_idx = subset_idx\n",
        "        else:\n",
        "            raise IndexError(f\"Subset index {subset_idx} out of range\")\n",
        "    \n",
        "    @property\n",
        "    def dataset(self):\n",
        "        return self.subsets[self.subset_idx]\n",
        "    \n",
        "    @property\n",
        "    def loader(self):\n",
        "        return self.subloaders[self.subset_idx]\n",
        "    \n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self.loader.batch_size\n",
        "\n",
        "    @classmethod\n",
        "    def from_filters(cls, dataset: Dataset, labels_per_subset: Tuple[Tuple[int, ...]], **kwargs):\n",
        "        \"\"\"\n",
        "        Returns a SubsetsLoader that trains on different subsets of the dataset \n",
        "        depending on the active subset.\n",
        "        \"\"\"\n",
        "        subsets = tuple(get_filtered_dataset(dataset, labels) for labels in labels_per_subset)\n",
        "        return cls(subsets, **kwargs)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return f\"SubsetsLoader({self.subsets}, batch_size={self.batch_size})\"\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.subloaders[self.subset_idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "subsets_loader = SubsetsLoader.from_filters(\n",
        "    mnist_train,\n",
        "    labels_per_subset=tuple(tuple(range(i+1)) for i in range(0, 10)),\n",
        "    batch_size=256,\n",
        "    shuffle=True, \n",
        ")\n",
        "\n",
        "subsets_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2254430003.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    labels_per_subset=tuple((i,) for i in range(0, 10)),,\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "per_label_train_loader = SubsetsLoader.from_filters(\n",
        "    mnist_train,\n",
        "    labels_per_subset=tuple((i,) for i in range(0, 10)),,\n",
        "    batch_size=256,\n",
        ")\n",
        "\n",
        "per_label_test_loader = SubsetsLoader.from_filters(\n",
        "    mnist_test,\n",
        "    labels_per_subset=tuple((i,) for i in range(0, 10)),,\n",
        "    batch_size=256,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "    def __init__(self, per_label_train_loader, per_label_test_loader, loss_fn):\n",
        "        self.per_label_train_loader = per_label_train_loader\n",
        "        self.per_label_test_loader = per_label_test_loader\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        self.train_loss = t.zeros(10)\n",
        "        self.train_accuracy = t.zeros(10)\n",
        "        self.test_loss = t.zeros(10)\n",
        "        self.test_accuracy = t.zeros(10)\n",
        "\n",
        "        self.trainset_sizes = t.Tensor([len(subset) for subset in per_label_train_loader.subsets])\n",
        "        self.testset_sizes = t.Tensor([len(subset) for subset in per_label_test_loader.subsets])\n",
        "\n",
        "    def measure(self, model: nn.Module):\n",
        "        with torch.no_grad():\n",
        "            # Loop over each specific-label-restricted subset of data\n",
        "            for l in tqdm(range(10), desc=\"Measuring metrics\"):\n",
        "                self.per_label_train_loader.to_subset(l)\n",
        "                self.per_label_test_loader.to_subset(l)\n",
        "\n",
        "                for i, (x, y) in enumerate(self.per_label_train_loader):\n",
        "                    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "                    y_pred = model(x)\n",
        "                    _, predicted = torch.max(y_pred.data, 1)\n",
        "                    self.train_accuracy[l] += (predicted == y).sum().item()\n",
        "                    self.train_loss[l] += self.loss_fn(y_pred, y).item()\n",
        "\n",
        "                self.train_accuracy[l] /= self.trainset_sizes[l]\n",
        "                self.train_loss[l] /= self.trainset_sizes[l]\n",
        "\n",
        "                for i, (x, y) in enumerate(self.per_label_test_loader):\n",
        "                    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "                    y_pred = model(x)\n",
        "                    _, predicted = torch.max(y_pred.data, 1)\n",
        "                    self.test_accuracy[l] += (predicted == y).sum().item()\n",
        "                    self.test_loss[l] += self.loss_fn(y_pred, y).item()\n",
        "\n",
        "                self.test_accuracy[l] /= self.testset_sizes[l]\n",
        "                self.test_loss[l] /= self.testset_sizes[l]\n",
        "\n",
        "    def reset(self):\n",
        "        self.train_loss = t.zeros(10)\n",
        "        self.train_accuracy = t.zeros(10)\n",
        "        self.test_loss = t.zeros(10)\n",
        "        self.test_accuracy = t.zeros(10)\n",
        "\n",
        "    @property\n",
        "    def total_train_size(self):\n",
        "        return self.trainset_sizes.sum() \n",
        "    \n",
        "    @property\n",
        "    def total_test_size(self):\n",
        "        return self.testset_sizes.sum()\n",
        "\n",
        "    @property\n",
        "    def total_train_loss(self):\n",
        "        return (self.train_loss * self.trainset_sizes).sum() / self.total_train_size\n",
        "    \n",
        "    @property\n",
        "    def total_test_loss(self):\n",
        "        return (self.test_loss * self.testset_sizes).sum() / self.total_test_size\n",
        "\n",
        "    @property\n",
        "    def total_train_accuracy(self):\n",
        "        return (self.train_accuracy * self.trainset_sizes).sum() / self.total_train_size\n",
        "    \n",
        "    @property\n",
        "    def total_test_accuracy(self):\n",
        "        return (self.test_accuracy * self.testset_sizes).sum() / self.total_test_size\n",
        "\n",
        "    def as_dict(self):\n",
        "        d = {\n",
        "            \"train/loss/total\": self.total_train_loss,\n",
        "            \"test/loss/total\": self.total_test_loss,\n",
        "            \"train/accuracy/total\": self.total_train_accuracy,\n",
        "            \"test/accuracy/total\": self.total_test_accuracy,\n",
        "        }\n",
        "\n",
        "        for l in range(10):\n",
        "            d[f\"train/loss/{l}\"] = self.train_loss[l]\n",
        "            d[f\"test/loss/{l}\"] = self.test_loss[l]\n",
        "            d[f\"train/accuracy/{l}\"] = self.train_accuracy[l]\n",
        "            d[f\"test/accuracy/{l}\"] = self.test_accuracy[l]\n",
        "\n",
        "        return d      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIEtfSdVWM1P"
      },
      "source": [
        "Now we check whether we still get good accuracy with only 0, 1 and 2 in the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCRR4gtvxzCe"
      },
      "source": [
        "So as we can see we still get good accuracy with less digits, althought there appears to be much more variation in the accuracy which was not expected, and that it takes longer to get to a good accuracy which doesnt make much sense to me. Now we will try to train this model to learn on 0s and 1s only, then we introduce 2 halfway into the second half of training (Epoch 50) to see if there is discontinuity in performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "bEYUIhm5x-r5",
        "outputId": "d7bd74dd-db69-45be-e9d8-6f20ad5a5e33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:2ko3oo6v) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy/0</td><td>▁█████████████████████▁</td></tr><tr><td>test/accuracy/1</td><td>▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█</td></tr><tr><td>test/accuracy/2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/6</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/7</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/8</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/9</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy/total</td><td>▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▃</td></tr><tr><td>test/loss/0</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄</td></tr><tr><td>test/loss/1</td><td>▂▇▇▇▇▇▇▇▇███████████▁▁▁</td></tr><tr><td>test/loss/2</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>test/loss/3</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>test/loss/4</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>test/loss/5</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>test/loss/6</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>test/loss/7</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▄▄</td></tr><tr><td>test/loss/8</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▃</td></tr><tr><td>test/loss/9</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>test/loss/total</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▃</td></tr><tr><td>train/accuracy/0</td><td>▁█████████████████████▁</td></tr><tr><td>train/accuracy/1</td><td>▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█</td></tr><tr><td>train/accuracy/2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/6</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/7</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/8</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/9</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy/total</td><td>▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▃</td></tr><tr><td>train/loss/0</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄</td></tr><tr><td>train/loss/1</td><td>▂▇▇▇▇▇▇▇▇███████████▁▁▁</td></tr><tr><td>train/loss/2</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/3</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/4</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/5</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/6</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/7</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▄▄</td></tr><tr><td>train/loss/8</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/9</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▄</td></tr><tr><td>train/loss/total</td><td>▁▆▆▇▇▇▇▇▇▇▇█████████▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy/0</td><td>0.0</td></tr><tr><td>test/accuracy/1</td><td>1.0</td></tr><tr><td>test/accuracy/2</td><td>0.0</td></tr><tr><td>test/accuracy/3</td><td>0.0</td></tr><tr><td>test/accuracy/4</td><td>0.0</td></tr><tr><td>test/accuracy/5</td><td>0.0</td></tr><tr><td>test/accuracy/6</td><td>0.0</td></tr><tr><td>test/accuracy/7</td><td>0.0</td></tr><tr><td>test/accuracy/8</td><td>0.0</td></tr><tr><td>test/accuracy/9</td><td>0.0</td></tr><tr><td>test/accuracy/total</td><td>0.1135</td></tr><tr><td>test/loss/0</td><td>0.93184</td></tr><tr><td>test/loss/1</td><td>0.51469</td></tr><tr><td>test/loss/2</td><td>6.81843</td></tr><tr><td>test/loss/3</td><td>6.82241</td></tr><tr><td>test/loss/4</td><td>6.8793</td></tr><tr><td>test/loss/5</td><td>6.81951</td></tr><tr><td>test/loss/6</td><td>6.90633</td></tr><tr><td>test/loss/7</td><td>6.86096</td></tr><tr><td>test/loss/8</td><td>6.84979</td></tr><tr><td>test/loss/9</td><td>6.84813</td></tr><tr><td>test/loss/total</td><td>5.55139</td></tr><tr><td>train/accuracy/0</td><td>0.0</td></tr><tr><td>train/accuracy/1</td><td>1.0</td></tr><tr><td>train/accuracy/2</td><td>0.0</td></tr><tr><td>train/accuracy/3</td><td>0.0</td></tr><tr><td>train/accuracy/4</td><td>0.0</td></tr><tr><td>train/accuracy/5</td><td>0.0</td></tr><tr><td>train/accuracy/6</td><td>0.0</td></tr><tr><td>train/accuracy/7</td><td>0.0</td></tr><tr><td>train/accuracy/8</td><td>0.0</td></tr><tr><td>train/accuracy/9</td><td>0.0</td></tr><tr><td>train/accuracy/total</td><td>0.11237</td></tr><tr><td>train/loss/0</td><td>0.93181</td></tr><tr><td>train/loss/1</td><td>0.51466</td></tr><tr><td>train/loss/2</td><td>6.8182</td></tr><tr><td>train/loss/3</td><td>6.82277</td></tr><tr><td>train/loss/4</td><td>6.87914</td></tr><tr><td>train/loss/5</td><td>6.81996</td></tr><tr><td>train/loss/6</td><td>6.90631</td></tr><tr><td>train/loss/7</td><td>6.86087</td></tr><tr><td>train/loss/8</td><td>6.85006</td></tr><tr><td>train/loss/9</td><td>6.84788</td></tr><tr><td>train/loss/total</td><td>5.55456</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">clear-wave-14</strong>: <a href=\"https://wandb.ai/jqhoogland/detecting-phase-transitions-sandbox/runs/2ko3oo6v\" target=\"_blank\">https://wandb.ai/jqhoogland/detecting-phase-transitions-sandbox/runs/2ko3oo6v</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230415_154220-2ko3oo6v/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2ko3oo6v). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a1da02c93c14f51a57ff6009506fee0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666912633333292, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.14.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/paperspace/Projects/detecting-phase-transitions/sandbox/wandb/run-20230415_160406-zm5w9xlg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jqhoogland/detecting-phase-transitions-sandbox/runs/zm5w9xlg\" target=\"_blank\">exalted-waterfall-15</a></strong> to <a href=\"https://wandb.ai/jqhoogland/detecting-phase-transitions-sandbox\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7205a2b89d68492f8d175465bd07841d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3d216560a5e4e939d7fe6a193b7c433",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40e7399773ee4cc5976ed816613b82d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b587260d46d84b52b50451fe08e81ecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50e9cf0bf9e74630b8c89edfe6e4f44d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a726ce215fd044349de11c27c1381554",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ddaa6f9e7aa4b7a893265e921372bdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48a26df8c0cb4dc4a2a2126decc6e245",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd0747e8221a4eaf9794d89046770d78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecc60212a9dc445d8f14a59df840e2fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b20372d06e8404d98533aaba773a1eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1980c3824b0f4c0c8134876871dc9466",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30babef867c04a4b9aa8b38cd9533344",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9587f04ee5a1469f9d55424373fc7c71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0954309c2f1a48bd8bda01320269254e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mini-batches:   0%|          | 0/5904 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "523e8aa0617c423998f31e9a60a4d687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "def train(model: nn.Module, loss_fn, optimizer, subsets_loader: SubsetsLoader, metrics: Metrics):\n",
        "    wandb.init(\"detecting-phase-transitions\")\n",
        "\n",
        "    epoch = 0\n",
        "    step = 0\n",
        "\n",
        "    subsets_loader.to_subset(0)\n",
        "\n",
        "    def measure(step):\n",
        "        metrics.reset()\n",
        "        metrics.measure(model)\n",
        "        wandb.log(metrics.as_dict(), step=step)\n",
        "\n",
        "    def save(step):\n",
        "        t.save(model.state_dict(), f\"./snapshots/model-{step}.pt\")\n",
        "    \n",
        "    # Epochs change in size, so we want to keep the number of steps constant between changes\n",
        "    for epoch in tqdm(range(100), desc=\"Epochs\"): \n",
        "        train_loss = 0.\n",
        "        \n",
        "        if epoch % 10 == 0 and step > 0:\n",
        "            subsets_loader.next_subset()\n",
        "            print(\"Next subset:\", subsets_loader.dataset)\n",
        "\n",
        "        measure(step)\n",
        "\n",
        "        # Round to the nearest multiple of 24\n",
        "        num_steps = len(subsets_loader)\n",
        "        num_steps = (num_steps // 24) * 24\n",
        "\n",
        "        # Training loop for mini-batches\n",
        "        for batch_idx, (x, y) in tqdm(enumerate(subsets_loader), desc=\"Mini-batches\", total=num_steps):            \n",
        "            if step % 24 == 0 and batch_idx > 0:\n",
        "                measure(step)\n",
        "                save(step)\n",
        "            \n",
        "            # Make predictions with the current parameters.\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            # Compute the loss value.\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update the parameters.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            if batch_idx == num_steps:\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "model = MNISTConvNet()\n",
        "loss_fn = nn.CrossEntropyLoss(size_average=False)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "metrics = Metrics(per_label_train_loader, per_label_test_loader, loss_fn)\n",
        "\n",
        "train(model, loss_fn, optimizer, subsets_loader, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Subset(MNIST, len=12665, labels=(0, 1))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subsets_loader.subsets[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.1502, -0.0863, -0.0904,  0.1373,  0.1703],\n",
              "          [-0.0899, -0.0035, -0.0838,  0.1226, -0.0249],\n",
              "          [ 0.0416,  0.0723, -0.1034, -0.1118,  0.0465],\n",
              "          [ 0.1931, -0.0500,  0.1327,  0.1058,  0.0363],\n",
              "          [ 0.0906, -0.1168, -0.0704, -0.0852,  0.0932]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1445,  0.1535, -0.1349, -0.1694, -0.1970],\n",
              "          [ 0.0468, -0.1261,  0.1544, -0.1707,  0.1580],\n",
              "          [-0.0618, -0.0103,  0.1685, -0.0978, -0.1937],\n",
              "          [ 0.1325, -0.1062, -0.1361, -0.1273, -0.1332],\n",
              "          [ 0.0038, -0.1336,  0.1817,  0.0996,  0.1449]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1107, -0.0946, -0.0011,  0.0466, -0.1486],\n",
              "          [-0.0622, -0.0422, -0.1251,  0.0103, -0.1843],\n",
              "          [ 0.0518, -0.1881,  0.0434, -0.0653,  0.1947],\n",
              "          [-0.0076,  0.1931,  0.1103, -0.0991,  0.0911],\n",
              "          [ 0.1207,  0.1462, -0.0592, -0.1234,  0.0493]]],\n",
              "\n",
              "\n",
              "        [[[-0.1774,  0.1225,  0.1136, -0.0555,  0.0371],\n",
              "          [-0.1898, -0.0470,  0.1703, -0.0228, -0.0592],\n",
              "          [-0.0825, -0.1988,  0.1407, -0.1026, -0.0749],\n",
              "          [-0.1383, -0.1841, -0.0420,  0.1796, -0.1435],\n",
              "          [-0.0588, -0.0485,  0.0856, -0.1261, -0.1976]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1450, -0.0128, -0.1092,  0.2011,  0.1264],\n",
              "          [ 0.0679,  0.0257, -0.0785, -0.0035, -0.0853],\n",
              "          [ 0.1868, -0.0197,  0.1740,  0.1845,  0.0843],\n",
              "          [-0.1034,  0.1139,  0.1604,  0.2112,  0.0352],\n",
              "          [-0.0271,  0.0798, -0.0645, -0.0431,  0.1870]]],\n",
              "\n",
              "\n",
              "        [[[-0.0951, -0.1674, -0.0869, -0.1327, -0.1833],\n",
              "          [-0.1842,  0.1520, -0.0557,  0.1477,  0.0120],\n",
              "          [-0.1517,  0.0822, -0.1861, -0.1674, -0.1461],\n",
              "          [ 0.1279,  0.0790, -0.0701, -0.1607, -0.1945],\n",
              "          [ 0.1335, -0.1289, -0.1387, -0.1081,  0.1464]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0074, -0.1325, -0.0881, -0.0020, -0.1927],\n",
              "          [-0.1296,  0.0870, -0.0931,  0.0495, -0.0193],\n",
              "          [-0.0772, -0.0311, -0.0101,  0.0325, -0.1357],\n",
              "          [-0.1645, -0.1095, -0.0373,  0.1238, -0.0007],\n",
              "          [ 0.0868,  0.1383, -0.0568, -0.1552,  0.0118]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1338,  0.1869,  0.1895,  0.1115,  0.1507],\n",
              "          [-0.0466,  0.1504,  0.1410, -0.0727,  0.0809],\n",
              "          [-0.0170, -0.0443, -0.0504, -0.0316, -0.0513],\n",
              "          [ 0.0600, -0.0140,  0.0747, -0.0794,  0.0775],\n",
              "          [-0.0343, -0.0149,  0.0996,  0.0826, -0.0824]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1223, -0.0805, -0.0927, -0.0405, -0.0329],\n",
              "          [-0.0030,  0.1504, -0.0964,  0.0149,  0.1896],\n",
              "          [-0.1456, -0.0550,  0.0008,  0.0953,  0.0175],\n",
              "          [ 0.1789,  0.1349, -0.0472,  0.0631, -0.1652],\n",
              "          [ 0.1729, -0.0733, -0.0169,  0.1134,  0.1321]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0163, -0.0072, -0.0185,  0.0221,  0.1517],\n",
              "          [ 0.0028, -0.1013,  0.0613, -0.1701,  0.1367],\n",
              "          [-0.1608,  0.1821,  0.1096, -0.0897,  0.0497],\n",
              "          [-0.1951,  0.0299, -0.1961, -0.1370,  0.0443],\n",
              "          [ 0.1802, -0.1894,  0.1308, -0.1611,  0.1471]]],\n",
              "\n",
              "\n",
              "        [[[-0.1848,  0.0567, -0.0565,  0.0863,  0.0388],\n",
              "          [ 0.1903,  0.1159, -0.0177, -0.0335, -0.0553],\n",
              "          [ 0.0094, -0.0315,  0.0966, -0.0369,  0.1461],\n",
              "          [-0.0625, -0.0151, -0.0588,  0.1993,  0.2155],\n",
              "          [ 0.1121,  0.2083, -0.0310,  0.0976, -0.1077]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1615,  0.1078,  0.1937,  0.1346,  0.1309],\n",
              "          [-0.0422,  0.0793,  0.0062,  0.0772,  0.0223],\n",
              "          [-0.1742,  0.0834,  0.0499,  0.0137, -0.0434],\n",
              "          [ 0.1962, -0.0119, -0.0841, -0.1738,  0.0854],\n",
              "          [-0.0061,  0.2003,  0.0524, -0.1015,  0.0711]]],\n",
              "\n",
              "\n",
              "        [[[-0.1063, -0.1701,  0.0661,  0.0346,  0.1769],\n",
              "          [ 0.1778,  0.1649,  0.1699,  0.0278,  0.2040],\n",
              "          [-0.1582,  0.0894, -0.0334, -0.0395,  0.0572],\n",
              "          [ 0.1589, -0.0848,  0.0621, -0.0113, -0.0528],\n",
              "          [-0.1751, -0.0910, -0.0272,  0.1846,  0.1015]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1642,  0.1954, -0.0944, -0.1879,  0.2099],\n",
              "          [-0.0685,  0.1122,  0.1850,  0.0256, -0.1813],\n",
              "          [-0.1367,  0.1655,  0.1643,  0.1559,  0.0354],\n",
              "          [ 0.0680, -0.1390,  0.0027,  0.0395, -0.0033],\n",
              "          [ 0.1473, -0.1288,  0.0540, -0.1885, -0.0477]]],\n",
              "\n",
              "\n",
              "        [[[-0.1830,  0.2008,  0.1707,  0.1432, -0.1089],\n",
              "          [-0.1182,  0.1437,  0.1687,  0.0775,  0.1028],\n",
              "          [-0.1700,  0.0493,  0.0808, -0.0616, -0.1459],\n",
              "          [-0.1625,  0.1936,  0.2159,  0.1267,  0.1353],\n",
              "          [ 0.0974,  0.1582,  0.0048,  0.0133, -0.0631]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0664, -0.1401, -0.1306,  0.0280, -0.1098],\n",
              "          [ 0.1129, -0.1776,  0.0068, -0.0445,  0.1578],\n",
              "          [-0.1491,  0.1648,  0.0008, -0.1671, -0.1186],\n",
              "          [ 0.0011,  0.0541,  0.1096, -0.1787, -0.0717],\n",
              "          [-0.1181,  0.1813, -0.1014,  0.1018,  0.1839]]]], device='cuda:0',\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
